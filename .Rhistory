vec<-c(df1[,1])
sentence <- c("a")
id <- c(0)
out2  <- cbind(sentence,id)
for(n in 1:784)
{
sentence <- convert_text_to_sentences(vec[n],lang = "en")
id <- n
out <- cbind(sentence,id)
out2 <- rbind(out2,out)
out2
}
head(out2[,1])
nrow(out2)
head(one_word,n=100)
head(two_word,n=40)
head(three_word,n=10)
out2[,1] <- str_to_lower(out2[,1])
out2[,1] <- str_trim(out2[,1], side = c("right"))
out2[,1] <- str_replace_all(out2[,1], "[[:punct:]]", " ")
out2[,1] <- gsub('[[:digit:]]+', '', out2[,1])
out2[,1] <- gsub("  ", " ", out2[,1])
topic1="service|servic|services|cable"
topic2="callback|call back"
topic3="dish"
topic4= "box|equipment"
topic5="picture|reception"
topic6= "channel|channels"
topic7="price|cost"
topic8="instal|installed|installer|instln|instlr"
topic9 = "internet"
topic10="no|not|never"
service <- numeric(953)
callback <- numeric(953)
dish <- numeric(953)
box <- numeric(953)
reception <- numeric(953)
channel <- numeric(953)
cost <- numeric(953)
installation <- numeric(953)
internet <- numeric(953)
indicator <- numeric(953)
for(i in 1:953)
{
service[i] <- grepl(topic1,out2[i,1])
callback[i] <- grepl(topic2,out2[i,1])
dish[i] <- grepl(topic3,out2[i,1])
box[i] <- grepl(topic4,out2[i,1])
reception[i] <- grepl(topic5,out2[i,1])
channel[i] <- grepl(topic6,out2[i,1])
cost[i] <- grepl(topic7,out2[i,1])
installation[i] <- grepl(topic8,out2[i,1])
internet[i] <- grepl(topic9,out2[i,1])
indicator[i] <- grepl(topic10,out2[i,1])
}
out3 <- data.frame(out2,service,callback,dish,box,reception,channel,cost,
installation,internet,indicator)
head(out3)
afinn_list <- read.delim(file='C:\\Users\\RR500204\\Desktop\\Session3\\Textmining in R\\AFINN-111.txt', header=FALSE, stringsAsFactors=FALSE)
names(afinn_list) <- c('word', 'score')
afinn_list$word <- tolower(afinn_list$word)
vNegTerms <- afinn_list$word[afinn_list$score==-5 | afinn_list$score==-4]
negTerms <- c(afinn_list$word[afinn_list$score==-3 | afinn_list$score==-2 | afinn_list$score==-1], "second-rate", "moronic", "third-rate", "flawed", "juvenile", "boring", "distasteful", "ordinary", "disgusting", "senseless", "static", "brutal", "confused", "disappointing", "bloody", "silly", "tired", "predictable", "stupid", "uninteresting", "trite", "uneven", "outdated", "dreadful", "bland")
vPosTerms <- c(afinn_list$word[afinn_list$score==5 | afinn_list$score==4], "uproarious", "riveting", "fascinating", "dazzling", "legendary")
posTerms <- c(afinn_list$word[afinn_list$score==3 | afinn_list$score==2 | afinn_list$score==1], "first-rate", "insightful", "clever", "charming", "comical", "charismatic", "enjoyable", "absorbing", "sensitive", "intriguing", "powerful", "pleasant", "surprising", "thought-provoking", "imaginative", "unpretentious")
vPosTerms <- c(afinn_list$word[afinn_list$score==5 | afinn_list$score==4], "uproarious", "riveting", "fascinating", "dazzling", "legendary")
sc <- function(sentence, vNegTerms, negTerms, posTerms, vPosTerms)
{
initial_sentence <- sentence
#remove unnecessary characters and split up by word
sentence <- gsub('[[:punct:]]', '', sentence)
sentence <- gsub('[[:cntrl:]]', '', sentence)
sentence <- gsub('\\d+', '', sentence)
sentence <- tolower(sentence)
wordList <- str_split(sentence, '\\s+')
words <- unlist(wordList)
#build vector with matches between sentence and each category
vPosMatches <- match(words, vPosTerms)
posMatches <- match(words, posTerms)
vNegMatches <- match(words, vNegTerms)
negMatches <- match(words, negTerms)
#sum up number of words in each category
vPosMatches <- sum(!is.na(vPosMatches))
posMatches <- sum(!is.na(posMatches))
vNegMatches <- sum(!is.na(vNegMatches))
negMatches <- sum(!is.na(negMatches))
score <- c(vNegMatches, negMatches, posMatches, vPosMatches)
return(score)
}
sentiment <- matrix(nrow=953,ncol=4)
for(i in 1:953)
{
sentiment[i,] <- sc(out2[i,1],vNegTerms, negTerms, posTerms, vPosTerms)
}
sentiment <- data.frame(sentiment)
names(sentiment) <- c("vNegTerms", "negTerms", "posTerms", "vPosTerms")
out <- data.frame(out3,sentiment)
fix(out)
head(sentiment)
nrow(out)
for(i in 1:953)
{
out$pos[i] <- sum(out$posTerms[i],out$vPosTerms[i])
out$neg[i] <- sum(out$negTerms[i],out$vNegTerms[i])
out$diff[i] <- out$pos[i]-out$neg[i]
out$reversal[i] <- ifelse((out$indicator[i]==1),-1,1)
out$senti[i] <- out$diff[i]*out$reversal[i]
}
out$sentiment[out$senti==0] <- "Neutral"
out$sentiment[out$senti>=1] <- "Positive"
out$sentiment[out$senti <= -1] <- "Negative"
fix(out)
write.table(out,"C:\\Users\\RR500204\\Desktop\\Session3\\Textmining in R\\output.csv",sep=",")
names(out)
mentions <- c(sum(out$service),sum(out$callback),sum(out$dish),
sum(out$box),sum(out$reception),sum(out$channel),sum(out$cost),
sum(out$installation),sum(out$internet))
topics <- c("service","callback","dish","box","reception","channel",
"cost","installation","internet")
buzz <- data.frame(topics,mentions)
buzz
service_sent <- table(out$service,out$sentiment)
service_sent
service_sent <- table(out$service,out$sentiment)
callback_sent <- table(out$callback,out$sentiment)
dish_sent <- table(out$dish,out$sentiment)
box_sent <- table(out$box,out$sentiment)
reception_sent <- table(out$reception,out$sentiment)
channel_sent <- table(out$channel,out$sentiment)
cost_sent <- table(out$cost,out$sentiment)
installation_sent <- table(out$installation,out$sentiment)
internet_sent <- table(out$internet,out$sentiment)
sentiment_summary <- rbind(service_sent[2,],callback_sent[2,],dish_sent[2,],
box_sent[2,],reception_sent[2,],channel_sent[2,],cost_sent[2,],
installation_sent[2,],internet_sent[2,])
sentiment_summary <- data.frame(topics,sentiment_summary)
sentiment_summary
wordcloud(words = buzz$topics, freq = buzz$mentions, min.freq = 1,
max.words=9, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
barplot(buzz$mentions, las = 2, names.arg = buzz$topics,
col ="lightblue", main ="Most frequent words",ylab = "Word frequencies")
wordcloud(words = buzz$topics, freq = buzz$mentions, min.freq = 1,
max.words=9, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
wordcloud(words = buzz$topics, freq = buzz$mentions, min.freq = 1,
max.words=9, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
wordcloud(words = buzz$topics, freq = buzz$mentions, min.freq = 1,
max.words=1, random.order=FALSE, rot.per=0.35,colors=brewer.pal(8, "Dark2"))
save.image("C:\\Users\\RR500204\\Desktop\\Session3\\TextMining.RData")
q()
library(tm)
library(wordcloud)
library(tau)
library(RTextTools)
library(RSentiment)
library(rJava)
data<-read.csv("C:\\Users\\RR500204\\Desktop\\Sentiment\\Email.csv", stringsAsFactors = FALSE,header=TRUE)
review_text<-paste(data$text,collapse=" ")
review_source<-VectorSource(review_text)
corpus<-Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
myStopwords<-c("can","nice","production","time","slides","job","project","work","instructions","also","bcg","times","one","result","make","sent","thanks","good","great","get")
corpus <- tm_map(corpus, removeWords, myStopwords)
dtm<-DocumentTermMatrix(corpus)
dtm2<-as.matrix(dtm)
frequency<-colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
write.csv(dtm2,file="C:\\Users\\RR500204\\Desktop\\Sentiment\\Output.csv",row.names=TRUE)
words<-names(frequency)
wordcloud(words[1:100],frequency[1:100],min.freq = 2,max.words=Inf)
#require(reshape2)
#melt(data)
data.frame(senti <- calculate_sentiment(data$text))
senti <- data.frame(senti)
Overall_sentiment <- calculate_total_presence_sentiment(data)
write.csv(senti,file="C:\\Users\\RR500204\\Desktop\\Sentiment\\Score.csv",row.names=TRUE)
write.csv(Overall_sentiment,file="C:\\Users\\RR500204\\Desktop\\Sentiment\\Score1.csv",row.names=TRUE)
install.packages("tau")
install.packages("RTextTools")
install.packages("RSentiment")
install.packages("rJava")
library(tm)
library(wordcloud)
library(tau)
library(RTextTools)
library(RSentiment)
library(rJava)
data<-read.csv("C:\\Users\\RR500204\\Desktop\\Sentiment\\Email.csv", stringsAsFactors = FALSE,header=TRUE)
review_text<-paste(data$text,collapse=" ")
review_source<-VectorSource(review_text)
corpus<-Corpus(review_source)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, stripWhitespace)
corpus <- tm_map(corpus, removeWords, stopwords("en"))
myStopwords<-c("can","nice","production","time","slides","job","project","work","instructions","also","bcg","times","one","result","make","sent","thanks","good","great","get")
corpus <- tm_map(corpus, removeWords, myStopwords)
dtm<-DocumentTermMatrix(corpus)
dtm2<-as.matrix(dtm)
frequency<-colSums(dtm2)
frequency <- sort(frequency, decreasing=TRUE)
write.csv(dtm2,file="C:\\Users\\RR500204\\Desktop\\Sentiment\\Output.csv",row.names=TRUE)
words<-names(frequency)
wordcloud(words[1:100],frequency[1:100],min.freq = 2,max.words=Inf)
#require(reshape2)
#melt(data)
data.frame(senti <- calculate_sentiment(data$text))
senti <- data.frame(senti)
Overall_sentiment <- calculate_total_presence_sentiment(data)
write.csv(senti,file="C:\\Users\\RR500204\\Desktop\\Sentiment\\Score.csv",row.names=TRUE)
write.csv(Overall_sentiment,file="C:\\Users\\RR500204\\Desktop\\Sentiment\\Score1.csv",row.names=TRUE)
install.packages("reshape2")
ATI <- read.table("C:\\Users\\RR500204\\Desktop\\Book2.csv", header=TRUE, sep=",")
Plotingdata<- data.frame(ATI)
#Plotingweek<- data.frame(Week)
#diameter <- qcc.groups(Plotingdata$ati,Plotingdata$Week)
fix(Plotingdata)
ATIchart<-(Plotingdata$ati)
Weekonchart<-(Plotingdata$Week)
jpeg(file="C:\\Users\\rr218830\\Desktop\\saving_plot1.jpeg")
qcc(Plotingdata[1:nrow(Plotingdata),], type="xbar",nsigmas=3,rules=shewhart.rules,title="Weekly ATI",xlab="Week",ylab="ATI",labels=Weekonchart)
Plotingweek<- data.frame(Week)
install.packages("qcc")
Weekonchart<-(Plotingdata$Week)
jpeg(file="C:\\Users\\rr218830\\Desktop\\saving_plot1.jpeg")
jpeg(file="C:\\Users\\RR500204\\Desktop\\saving_plot1.jpeg")
qcc(Plotingdata[1:nrow(Plotingdata),], type="xbar",nsigmas=3,rules=shewhart.rules,title="Weekly ATI",xlab="Week",ylab="ATI",labels=Weekonchart)
install.packages("tm.plugin.mail", lib="C:/Program Files/R/R-3.3.2/library")
library(purrr)
x = as.integer(readline(prompt="Enter a number: "))
fractional_reduce<-reduce(c(seq(1:x)), function(x,y){
x*y
})
fractional_reduce(5)
fractional_reduce<-reduce(c(seq(1:x)), function(x,y){
x*y
})
fractional_reduce(5)
x = as.integer(readline(prompt="Enter a number: "))
fractional_reduce<-reduce(c(seq(1:x)), function(x,y){
x*y
})
Inputnum = as.integer(readline(prompt="Enter a number: "))
Factorial_loop = 1
# To check if a factorial can be calculated
if(Inputnum < 0) {
print("Factorials cannot be computed for negative numbers")
} else if(Inputnum == 0) {
print("The factorial of 0 is 1")
} else {
for(i in 1:Inputnum) {
Factorial_loop = Factorial_loop * i
}
print(paste("The factorial of", Inputnum ,"is",Factorial_loop))
}
Factorial_func <- function(Inputnum) {
if (Inputnum == 0)    return (1)
else  return (Inputnum * Factorial_func(Inputnum-1))
}
Factorial_func(5)
library(swirl)
swirl()
library(rattle)
install.packages("rattle", lib="C:/Program Files/R/R-3.3.2/library")
library(rattle)
library(rattle)
rattle()
---
title: "TestMarkDown"
author: "Michelle"
date: "February 20, 2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## R Markdown
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r cars}
summary(cars)
```
## Including Plots
You can also embed plots, for example:
```{r pressure, echo=FALSE}
plot(pressure)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
title: "TestMarkDown"
---
title: "TestMarkDown"
author: "Michelle"
date: "February 20, 2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## R Markdown
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r cars}
summary(cars)
```
## Including Plots
You can also embed plots, for example:
```{r pressure, echo=FALSE}
plot(pressure)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
---
title: "TestMarkDown"
author: "Michelle"
date: "February 20, 2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
## R Markdown
This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.
When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
```{r cars}
summary(cars)
```
## Including Plots
You can also embed plots, for example:
```{r pressure, echo=FALSE}
plot(pressure)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
rattle()
library("devtools")
library("testthat")
use_travis()
library(CourseraAssignment)
use_travis()
devtools::check()
library(CourseraAssignment)
devtools::check()
library(devtools)
devtools::check()
library(dplyr)
library("tidyr")
library(maps)
setwd("C:\\Users\\RR500204\\Desktop\\data\\Assignment 4_Resubmit")
#'Displaying year-wise fatality summaries and representing them visually in the form of maps by
#'state
#'
#'The dataset used data from the US National Highway Traffic Safety Administration's
#'     Fatality Analysis Reporting System, which is a nationwide census providing the
#'     American public yearly data regarding fatal injuries suffered in motor vehicle
#'     traffic crashes
#'
#'  The function fars_read is used to evaluate the existance of a file and read in the
#'  information into a dataframe if the file exists
#'
#'
#'  @param to print the message saying the input file name does not exist if the file does not
#'  @param exist else it reads in csv file that is provided as input. All messages pertaining
#'    to the progress of the reading in of the csv file is suppresed. A dataframe is created to
#'    save the input that is read in
#'
#'  @return This function either returns a message the filename does not exist if the file name
#'     name does not exist or a dataframe containing the information if the file exists
#'
#'
#'
#'  @examples
#'  fars_read("accident_2013.csv.bz2")
#'  fars_read("accident_2017.csv.bz2")
#'
#'
#'  The make_filename function is used to create a filename using the year as an input and
#'  contantenating the text "accident",the year that was inputted and the extensions ".csv.bz2"
#'
#'  @param to print the filename by contantenating the text "accident",the numeric value of the
#'    year that was inputted and the extensions ".csv.bz2"
#'
#'  @return This function returns a character vector containing the formatted combination of text
#'     and variable values
#'
#'  @examples
#'  make_filename(2013)
#'  make_filename(2014)
#'
#'
#' The function fars_read_years is used to take the year as an input and create a filename
#' using make_filename function. The file is then read using the fars_read function. To which a
#' column called year is added using the mutate function. To use the mutate function the dplyr
#' package needs to be installed.The data is then subset by month and
#' year using the dplyr::select function. Trycatch provides a mechanism for handling unusual
#' conditions. If an error is generated, the error handler picks up the error generated and
#' displays a warning message "invalid year: " with the invalid year information. If no error is
#' generated, it displays "NULL". The entire function of creating the files, reading in the
#' details, adding a year column, subsetting the information is applied to all elements of the
#' list and returns a list using the lapply function
#'
#' @param takes a year as input,creates a filename, reads the information, adds a column for year then
#'    subsets the information, validates if there is an error and appropriately displays a warning
#'    message if an error occurs else returns "NULL"
#'
#'
#' @return If an error is generated a warning message "invalid year: " with
#'    the invalid year information is displayed. If no error is generated it displays "NULL"
#'
#' @examples
#' fars_read_years(2013)
#' fars_read_years(2014)
#'
#'
#' The fars_summarize_years function takes a year as an input and summarizes the number of
#' accidents by month within the input year specified
#'
#' @param to display a monthwise summary of the number of fatal injuries for the year
#'  specified
#'
#' @return A table displaying a month number and the consolidated number of fatal injuries for
#' each month of the specified year
#'
#' @examples
#' fars_summarize_years(2013)
#' fars_summarize_years(2014)
#'
#'
#'The fars_map_state function takes the state number and year as an input and displays a plot
#'of the state with the number of fatalaties mapped as dots at the appropriate location of the
#'incident. To be able to use this function ensure the maps package is installed
#'
#' @param To display a map plot of the state for the year specified with the dots for each fatal
#'    injury incident. Each state has a state number corresponding to it. The input to this
#'    function is the state number and the year. The data for the state is filtered and the
#'    number of rows is determined. Each fatal injury incident is plotted as dot within the map
#'    at their corresponding latitude and longitude
#'
#'
#' @importFrom graphics points
#'
#' @return This function returns map plot of the state for the year specified with the dots for
#'    each fatal injury incident with the map at their corresponding latitude and longitude of
#'    occurence
#'
#'
#' @examples
#' fars_map_state(1,2013)
#' fars_map_state(4,2013)
#'
#'
#' @export
fars_read <- function(filename) {
if(!file.exists(filename))
stop("file '", filename, "' does not exist")
data <- suppressMessages({
readr::read_csv(filename, progress = FALSE)
})
dplyr::tbl_df(data)
}
#' @export
make_filename <- function(year) {
year <- as.integer(year)
sprintf("accident_%d.csv.bz2", year)
}
#' @export
fars_read_years <- function(years) {
lapply(years, function(year) {
file <- make_filename(year)
tryCatch({
dat <- fars_read(file)
dplyr::mutate(dat, year = year) %>%
dplyr::select(MONTH, year)
}, error = function(e) {
warning("invalid year: ", year)
return(NULL)
})
})
}
#' @export
fars_summarize_years <- function(years) {
dat_list <- fars_read_years(years)
dplyr::bind_rows(dat_list) %>%
dplyr::group_by(year, MONTH) %>%
dplyr::summarize(n = n()) %>%
tidyr::spread(year, n)
}
fars_map_state <- function(state.num, year) {
filename <- make_filename(year)
data <- fars_read(filename)
state.num <- as.integer(state.num)
if(!(state.num %in% unique(data$STATE)))
stop("invalid STATE number: ", state.num)
data.sub <- dplyr::filter(data, STATE == state.num)
if(nrow(data.sub) == 0L) {
message("no accidents to plot")
return(invisible(NULL))
}
is.na(data.sub$LONGITUD) <- data.sub$LONGITUD > 900
is.na(data.sub$LATITUDE) <- data.sub$LATITUDE > 90
with(data.sub, {
maps::map("state", ylim = range(LATITUDE, na.rm = TRUE),
xlim = range(LONGITUD, na.rm = TRUE))
graphics::points(LONGITUD, LATITUDE, pch = 46)
})
}
library("CourseraAssignment", lib.loc="~/R/win-library/3.3")
devtools::check()
install.packages("devtools", lib="C:/Program Files/R/R-3.3.2/library")
library(devtools)
devtools::check()
library(devtools)
library(devtools)
devtools::check()
